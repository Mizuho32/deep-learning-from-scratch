= ゼロからつくる Deep Learning

== 1章
Python

== 2章
パーセプトロン。AND, OR, XORの恒例のやつ。

== 3章
ニューラルネットワーク。

3.2.1より、パーセプトロンとニューラルネットワークの違いは活性化関数だけ、ということになっているらしい

softmaxの計算には注意。

Juliaではargmax(x)-1とする

== 4章

損失関数を定義して、損失が小さくなるように重みで微分して重みを最適化

== 5章
計算木が大変。
値が行列になった時に逆伝播する値をバッチの個数で割ったりsumしたりするところが直観に反する。
式を書けばわかる・・・?

== 6章

学習をうまいことやりたい。

最適化アルゴリズム::
- SGD
- Momentum
- AdaGrad
- Adam
初期値::
- Xavier
- He
BatchNormalization::
各層のアクティベーションが良い分布になるように無理やりNormalizeするレイヤを挿入。
正則化::
- Weight decay, 損失関数にパラメータを加える。(自動微分?)
- Dropout
ハイパパラメータ::
- 検証データ。ハイパーパラメータ専用データ

== 7章
CNNの実装面倒

== 8章

AlexNet::
ブームの火付け役
GoogLeNet::
コンボリューション層を縦でなく横にも
ResNet::
スキップ機能
分散学習::
Tensorflowの技術論文
ビット削減::
16bitでも事足りる。Binarized Neural Networks。
物体検出::
R-CNN, Faster R-CNN
セグメンテーション::
FCN、全部コンボリューション。
走路環境認識::
SegNet
