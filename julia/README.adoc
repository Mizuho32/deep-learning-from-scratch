= ゼロからつくる Deep Learning

== 1章
Python

== 2章
パーセプトロン。AND, OR, XORの恒例のやつ。

== 3章
ニューラルネットワーク。

3.2.1より、パーセプトロンとニューラルネットワークの違いは活性化関数だけ、ということになっているらしい

softmaxの計算には注意。

Juliaではargmax(x)-1とする

== 4章

損失関数を定義して、損失が小さくなるように重みで微分して重みを最適化

== 5章
計算木が大変。
値が行列になった時に逆伝播する値をバッチの個数で割ったりsumしたりするところが直観に反する。
式を書けばわかる・・・?

== 6章

学習をうまいことやりたい。

最適化アルゴリズム::
- SGD
- Momentum
- AdaGrad
- Adam
初期値::
- Xavier
- He
BatchNormalization::
各層のアクティベーションが良い分布になるように無理やりNormalizeするレイヤを挿入。
正則化::
- Weight decay, 損失関数にパラメータを加える。(自動微分?)
- Dropout
ハイパパラメータ::
- 検証データ。ハイパーパラメータ専用データ
